{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stupid-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets.mlm_ft_dataset import MLMFTDataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForMaskedLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from readers import lenta_reader, ria_reader, tg_reader\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "listed-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/aobuhtijarov/models/rubert_cased_L-12_H-768_A-12_pt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extra-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "critical-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file /home/aobuhtijarov/models/rubert_cased_L-12_H-768_A-12_pt/added_tokens.json. We won't load it.\n",
      "Didn't find file /home/aobuhtijarov/models/rubert_cased_L-12_H-768_A-12_pt/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /home/aobuhtijarov/models/rubert_cased_L-12_H-768_A-12_pt/tokenizer_config.json. We won't load it.\n",
      "Didn't find file /home/aobuhtijarov/models/rubert_cased_L-12_H-768_A-12_pt/tokenizer.json. We won't load it.\n",
      "loading file /home/aobuhtijarov/models/rubert_cased_L-12_H-768_A-12_pt/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file /home/aobuhtijarov/models/rubert_cased_L-12_H-768_A-12_pt/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.5.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file /home/aobuhtijarov/models/rubert_cased_L-12_H-768_A-12_pt/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/aobuhtijarov/models/rubert_cased_L-12_H-768_A-12_pt/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at /home/aobuhtijarov/models/rubert_cased_L-12_H-768_A-12_pt/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(119547, 768, padding_idx=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=False, do_basic_tokenize=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "missing-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-reason",
   "metadata": {},
   "source": [
    "## TG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rental-transmission",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "484037it [04:55, 1638.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "484037"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg_records = [r for r in tqdm.tqdm(tg_reader('/home/aobuhtijarov/datasets/telegram_news/ru_tg_1101_0510.jsonl'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "freelance-texture",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120050it [01:14, 1604.76it/s]\n"
     ]
    }
   ],
   "source": [
    "tg_records.extend(\n",
    "    [r for r in tqdm.tqdm(tg_reader('/home/aobuhtijarov/datasets/telegram_news/ru_tg_0511_0517.jsonl'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "instrumental-procurement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604087"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = MLMFTDataset([t['text'] for t in tg_records], tokenizer)\n",
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-appreciation",
   "metadata": {},
   "source": [
    "## LentaRIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "descending-inspector",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75971it [00:03, 24049.66it/s]\n",
      "858741it [10:59, 1302.21it/s]\n"
     ]
    }
   ],
   "source": [
    "lenta_records = [r for r in tqdm.tqdm(lenta_reader('/home/aobuhtijarov/datasets/lenta/lenta-ru-news.val.csv'))]\n",
    "\n",
    "\n",
    "ria_records = [r for r in tqdm.tqdm(ria_reader(\n",
    "    '/home/aobuhtijarov/datasets/ria/ria.shuffled.train.json'))]\n",
    "\n",
    "lenta_records = [r for r in lenta_records if r['date'][:4] in ['2010', '2011', '2012', '2013', '2014']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "normal-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = MLMFTDataset([t['text'] for t in lenta_records + ria_records], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-queensland",
   "metadata": {},
   "source": [
    "## Split and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lonely-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraq = 0.97\n",
    "\n",
    "train_size = int(train_fraq * len(full_dataset))\n",
    "test_size = int((1-train_fraq) * 0.5 * len(full_dataset))\n",
    "\n",
    "train_dataset, test_dataset, eval_dataset = \\\n",
    "    torch.utils.data.random_split(full_dataset, [train_size, test_size, len(full_dataset) - train_size - test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "minus-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: leshanbog (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.29<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">RuBERT TG fine tuning on text</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/leshanbog/master-thesis\" target=\"_blank\">https://wandb.ai/leshanbog/master-thesis</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/leshanbog/master-thesis/runs/2vqbydip\" target=\"_blank\">https://wandb.ai/leshanbog/master-thesis/runs/2vqbydip</a><br/>\n",
       "                Run data is saved locally in <code>/home/aobuhtijarov/master-thesis/src/wandb/run-20210507_081853-2vqbydip</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project='master-thesis', name='RuBERT TG fine tuning on text');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "concerned-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.config.per_device_train_batch_size = 4\n",
    "wandb.run.config.gradient_accumulation_steps = 16\n",
    "wandb.run.config.learning_rate = 3e-5\n",
    "wandb.run.config.warmup_steps = 500\n",
    "wandb.run.config.logging_steps = 25\n",
    "wandb.run.config.eval_steps = 100\n",
    "wandb.run.config.save_steps = 100\n",
    "wandb.run.config.max_steps = 2000\n",
    "wandb.run.config.weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "characteristic-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert_ft_on_tg_text',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=wandb.run.config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=wandb.run.config.gradient_accumulation_steps,\n",
    "    evaluation_strategy='steps',\n",
    "    learning_rate=wandb.run.config.learning_rate,\n",
    "    warmup_steps=wandb.run.config.warmup_steps,\n",
    "    overwrite_output_dir=False,\n",
    "    logging_steps=wandb.run.config.logging_steps,\n",
    "    eval_steps=wandb.run.config.eval_steps,\n",
    "    save_steps=wandb.run.config.save_steps,\n",
    "    max_steps=wandb.run.config.max_steps,\n",
    "    save_total_limit=1,\n",
    "    weight_decay=wandb.run.config.weight_decay,\n",
    "    report_to='wandb',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "conventional-talent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-material",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 585964\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 2000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='54' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  54/2000 04:19 < 2:41:35, 0.20 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.summary.update({'Test eval': res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
